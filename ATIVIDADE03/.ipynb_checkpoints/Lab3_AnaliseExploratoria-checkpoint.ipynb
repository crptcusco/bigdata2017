{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![CMCC](http://cmcc.ufabc.edu.br/images/logo_site.jpg)\n",
    "# ** Análise Exploratória **\n",
    "\n",
    "#### Esse notebook introduz os conceitos de Análise Exploratória\n",
    "\n",
    "#### Para isso utilizaremos a base de dados de [Crimes de São Francisco](https://www.kaggle.com/c/sf-crime) obtidos do site de competições [Kaggle](https://www.kaggle.com/).\n",
    "\n",
    "#### ** Esse notebook contém:  **\n",
    "#### *Parte 1:* *Parsing* da base de dados de Crimes de São Francisco\n",
    "#### *Parte 2:* Estatísticas Básicas das Variáveis\n",
    "#### *Parte 3:* Plotagem de Gráficos\n",
    "\n",
    "#### Para os exercícios é aconselhável consultar a documentação da [API do PySpark](https://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### ** Parte 1: Parsing da Base de Dados **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Nessa primeira parte do notebook vamos aprender a trabalhar com arquivos CSV. Os arquivos CSV são arquivos textos representando tabelas de dados, numéricas ou categóricas, com formatação apropriada para a leitura estruturada.\n",
    "\n",
    "#### A primeira linha de um arquivo CSV é o cabeçalho, com o nome de cada coluna da tabela separados por vírgulas.\n",
    "\n",
    "#### Cada linha subsequente representa um objeto da base de dados com os valores também separados por vírgula. Esses valores podem ser numéricos, categóricos (textuais) e listas. As listas são representadas por listas de valores separadas por vírgulas e entre aspas.\n",
    "\n",
    "#### Vamos carregar a base de dados histórica de Crimes de São Francisco, um dos temas do projeto final. No primeiro passo vamos armazenar o cabeçalho em uma variável chamada `header` e imprimi-la para a descrição dos campos de nossa base."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Campos disponíveis: Dates,Category,Descript,DayOfWeek,PdDistrict,Resolution,Address,X,Y\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "from pyspark import SparkContext\n",
    "sc = SparkContext.getOrCreate()\n",
    "\n",
    "filename = os.path.join(\"Data\",\"Aula03\",\"Crime.csv\")\n",
    "CrimeRDD = sc.textFile(\"train.csv\",8)\n",
    "header = CrimeRDD.take(1)[0] # o cabeçalho é a primeira linha do arquivo\n",
    "\n",
    "print \"Campos disponíveis: {}\".format(header)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Durante os exercícios precisaremos pular a linha do cabeçalho de tal forma a trabalhar apenas com a tabela de dados.\n",
    "\n",
    "#### Uma forma de fazer isso é utilizando o comando `filter()` para eliminar toda linha igual a variável `header`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2015-05-13 23:53:00,WARRANTS,WARRANT ARREST,Wednesday,NORTHERN,\"ARREST, BOOKED\",OAK ST / LAGUNA ST,-122.425891675136,37.7745985956747\n"
     ]
    }
   ],
   "source": [
    "# EXERCICIO\n",
    "CrimeHeadlessRDD = CrimeRDD.filter(lambda x : x != header )\n",
    "\n",
    "firstObject = CrimeHeadlessRDD.take(1)[0]\n",
    "print firstObject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK\n"
     ]
    }
   ],
   "source": [
    "assert firstObject==u'2015-05-13 23:53:00,WARRANTS,WARRANT ARREST,Wednesday,NORTHERN,\"ARREST, BOOKED\",OAK ST / LAGUNA ST,-122.425891675136,37.7745985956747', 'valor incorreto'\n",
    "print \"OK\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Agora temos um dataset em que cada linha é uma string contendo todos os valores. Porém, para explorarmos os dados precisamos que cada objeto seja uma lista de valores.\n",
    "\n",
    "#### Utilize o comando `split()` para transformar os objetos em listas de strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'2015-05-13 23:53:00', u'WARRANTS', u'WARRANT ARREST', u'Wednesday', u'NORTHERN', u'\"ARREST', u' BOOKED\"', u'OAK ST / LAGUNA ST', u'-122.425891675136', u'37.7745985956747']\n"
     ]
    }
   ],
   "source": [
    "# EXERCICIO\n",
    "AUXRDD = (CrimeRDD\n",
    "          .filter(lambda x: x!=header)\n",
    "          .map(lambda x : x.split(\",\"))\n",
    "         )\n",
    "\n",
    "firstObjectList = AUXRDD.take(1)[0]\n",
    "print firstObjectList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK\n"
     ]
    }
   ],
   "source": [
    "assert firstObjectList[0]==u'2015-05-13 23:53:00', 'valores incorretos'\n",
    "print \"OK\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reparem que o campo *Resolution* cujo valor no primeiro registro era \"ARREST, BOOKED\" se tornou dois campos diferentes por causa do `split()`.\n",
    "\n",
    "#### Nesses casos em que uma simples separação não funciona, nós podemos utilizar as [Expressões Regulares](http://www.rexegg.com/regex-quickstart.html). O Python tem suporte as Regex através da biblioteca `re`. Vamos utilizar o comando [`re.split()`](https://docs.python.org/2/library/re.html#re.split) para cuidar da separação de nossa base em campos.\n",
    "\n",
    "#### Além disso, vamos aproveitar para converter o primeiro campo, que representa data e hora, para objeto do tipo [`datetime`](https://docs.python.org/2/library/datetime.html) através do comando `datetime.datetime.strptime()`. Também vamos agrupar as coordenadas X e Y em uma tupla de floats.\n",
    "\n",
    "#### Outra ajuda que o Python pode nos dar é a utilização das [`namedtuple`](https://docs.python.org/2/library/collections.html#namedtuple-factory-function-for-tuples-with-named-fields) que permite acessar cada campo de cada objeto pelo nome. Ex.: rec.Dates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/pyspark/cloudpickle.py\", line 148, in dump\n",
      "    return Pickler.dump(self, obj)\n",
      "  File \"/usr/lib/python2.7/pickle.py\", line 224, in dump\n",
      "    self.save(obj)\n",
      "  File \"/usr/lib/python2.7/pickle.py\", line 286, in save\n",
      "    f(self, obj) # Call unbound method with explicit self\n",
      "  File \"/usr/lib/python2.7/pickle.py\", line 568, in save_tuple\n",
      "    save(element)\n",
      "  File \"/usr/lib/python2.7/pickle.py\", line 286, in save\n",
      "    f(self, obj) # Call unbound method with explicit self\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/pyspark/cloudpickle.py\", line 255, in save_function\n",
      "    self.save_function_tuple(obj)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/pyspark/cloudpickle.py\", line 292, in save_function_tuple\n",
      "    save((code, closure, base_globals))\n",
      "  File \"/usr/lib/python2.7/pickle.py\", line 286, in save\n",
      "    f(self, obj) # Call unbound method with explicit self\n",
      "  File \"/usr/lib/python2.7/pickle.py\", line 554, in save_tuple\n",
      "    save(element)\n",
      "  File \"/usr/lib/python2.7/pickle.py\", line 286, in save\n",
      "    f(self, obj) # Call unbound method with explicit self\n",
      "  File \"/usr/lib/python2.7/pickle.py\", line 606, in save_list\n",
      "    self._batch_appends(iter(obj))\n",
      "  File \"/usr/lib/python2.7/pickle.py\", line 639, in _batch_appends\n",
      "    save(x)\n",
      "  File \"/usr/lib/python2.7/pickle.py\", line 286, in save\n",
      "    f(self, obj) # Call unbound method with explicit self\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/pyspark/cloudpickle.py\", line 255, in save_function\n",
      "    self.save_function_tuple(obj)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/pyspark/cloudpickle.py\", line 292, in save_function_tuple\n",
      "    save((code, closure, base_globals))\n",
      "  File \"/usr/lib/python2.7/pickle.py\", line 286, in save\n",
      "    f(self, obj) # Call unbound method with explicit self\n",
      "  File \"/usr/lib/python2.7/pickle.py\", line 554, in save_tuple\n",
      "    save(element)\n",
      "  File \"/usr/lib/python2.7/pickle.py\", line 286, in save\n",
      "    f(self, obj) # Call unbound method with explicit self\n",
      "  File \"/usr/lib/python2.7/pickle.py\", line 606, in save_list\n",
      "    self._batch_appends(iter(obj))\n",
      "  File \"/usr/lib/python2.7/pickle.py\", line 639, in _batch_appends\n",
      "    save(x)\n",
      "  File \"/usr/lib/python2.7/pickle.py\", line 286, in save\n",
      "    f(self, obj) # Call unbound method with explicit self\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/pyspark/cloudpickle.py\", line 255, in save_function\n",
      "    self.save_function_tuple(obj)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/pyspark/cloudpickle.py\", line 292, in save_function_tuple\n",
      "    save((code, closure, base_globals))\n",
      "  File \"/usr/lib/python2.7/pickle.py\", line 286, in save\n",
      "    f(self, obj) # Call unbound method with explicit self\n",
      "  File \"/usr/lib/python2.7/pickle.py\", line 554, in save_tuple\n",
      "    save(element)\n",
      "  File \"/usr/lib/python2.7/pickle.py\", line 286, in save\n",
      "    f(self, obj) # Call unbound method with explicit self\n",
      "  File \"/usr/lib/python2.7/pickle.py\", line 606, in save_list\n",
      "    self._batch_appends(iter(obj))\n",
      "  File \"/usr/lib/python2.7/pickle.py\", line 642, in _batch_appends\n",
      "    save(tmp[0])\n",
      "  File \"/usr/lib/python2.7/pickle.py\", line 286, in save\n",
      "    f(self, obj) # Call unbound method with explicit self\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/pyspark/cloudpickle.py\", line 249, in save_function\n",
      "    self.save_function_tuple(obj)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/pyspark/cloudpickle.py\", line 297, in save_function_tuple\n",
      "    save(f_globals)\n",
      "  File \"/usr/lib/python2.7/pickle.py\", line 286, in save\n",
      "    f(self, obj) # Call unbound method with explicit self\n",
      "  File \"/usr/lib/python2.7/pickle.py\", line 655, in save_dict\n",
      "    self._batch_setitems(obj.iteritems())\n",
      "  File \"/usr/lib/python2.7/pickle.py\", line 687, in _batch_setitems\n",
      "    save(v)\n",
      "  File \"/usr/lib/python2.7/pickle.py\", line 306, in save\n",
      "    rv = reduce(self.proto)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/pyspark/rdd.py\", line 204, in __getnewargs__\n",
      "    \"It appears that you are attempting to broadcast an RDD or reference an RDD from an \"\n",
      "Exception: It appears that you are attempting to broadcast an RDD or reference an RDD from an action or transformation. RDD transformations and actions can only be invoked by the driver, not inside of other transformations; for example, rdd1.map(lambda x: rdd2.values.count() * x) is invalid because the values transformation and count action cannot be performed inside of the rdd1.map transformation. For more information, see SPARK-5063.\n"
     ]
    },
    {
     "ename": "PicklingError",
     "evalue": "Could not serialize object: Exception: It appears that you are attempting to broadcast an RDD or reference an RDD from an action or transformation. RDD transformations and actions can only be invoked by the driver, not inside of other transformations; for example, rdd1.map(lambda x: rdd2.values.count() * x) is invalid because the values transformation and count action cannot be performed inside of the rdd1.map transformation. For more information, see SPARK-5063.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPicklingError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-56d5a8c114e7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     41\u001b[0m                     )\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m \u001b[0mfirstClean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCrimeHeadlessRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0mtotalRecs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCrimeHeadlessRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0mfirstClean\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/pyspark/rdd.pyc\u001b[0m in \u001b[0;36mtake\u001b[0;34m(self, num)\u001b[0m\n\u001b[1;32m   1341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1342\u001b[0m             \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartsScanned\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartsScanned\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnumPartsToTry\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotalParts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1343\u001b[0;31m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunJob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtakeUpToNumLeft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1344\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1345\u001b[0m             \u001b[0mitems\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/pyspark/context.pyc\u001b[0m in \u001b[0;36mrunJob\u001b[0;34m(self, rdd, partitionFunc, partitions, allowLocal)\u001b[0m\n\u001b[1;32m    990\u001b[0m         \u001b[0;31m# SparkContext#runJob.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    991\u001b[0m         \u001b[0mmappedRDD\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartitionFunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 992\u001b[0;31m         \u001b[0mport\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunJob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmappedRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpartitions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    993\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mport\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmappedRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    994\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/pyspark/rdd.pyc\u001b[0m in \u001b[0;36m_jrdd\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2453\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2454\u001b[0m         wrapped_func = _wrap_function(self.ctx, self.func, self._prev_jrdd_deserializer,\n\u001b[0;32m-> 2455\u001b[0;31m                                       self._jrdd_deserializer, profiler)\n\u001b[0m\u001b[1;32m   2456\u001b[0m         python_rdd = self.ctx._jvm.PythonRDD(self._prev_jrdd.rdd(), wrapped_func,\n\u001b[1;32m   2457\u001b[0m                                              self.preservesPartitioning)\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/pyspark/rdd.pyc\u001b[0m in \u001b[0;36m_wrap_function\u001b[0;34m(sc, func, deserializer, serializer, profiler)\u001b[0m\n\u001b[1;32m   2386\u001b[0m     \u001b[0;32massert\u001b[0m \u001b[0mserializer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"serializer should not be empty\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2387\u001b[0m     \u001b[0mcommand\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprofiler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdeserializer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mserializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2388\u001b[0;31m     \u001b[0mpickled_command\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbroadcast_vars\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mincludes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_prepare_for_python_RDD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2389\u001b[0m     return sc._jvm.PythonFunction(bytearray(pickled_command), env, includes, sc.pythonExec,\n\u001b[1;32m   2390\u001b[0m                                   sc.pythonVer, broadcast_vars, sc._javaAccumulator)\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/pyspark/rdd.pyc\u001b[0m in \u001b[0;36m_prepare_for_python_RDD\u001b[0;34m(sc, command)\u001b[0m\n\u001b[1;32m   2372\u001b[0m     \u001b[0;31m# the serialized command will be compressed by broadcast\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2373\u001b[0m     \u001b[0mser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCloudPickleSerializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2374\u001b[0;31m     \u001b[0mpickled_command\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2375\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpickled_command\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m<<\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# 1M\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2376\u001b[0m         \u001b[0;31m# The broadcast will have same life cycle as created PythonRDD\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/pyspark/serializers.pyc\u001b[0m in \u001b[0;36mdumps\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdumps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 460\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcloudpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/pyspark/cloudpickle.pyc\u001b[0m in \u001b[0;36mdumps\u001b[0;34m(obj, protocol)\u001b[0m\n\u001b[1;32m    702\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m     \u001b[0mcp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCloudPickler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 704\u001b[0;31m     \u001b[0mcp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    705\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    706\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/pyspark/cloudpickle.pyc\u001b[0m in \u001b[0;36mdump\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    160\u001b[0m                 \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Could not serialize object: %s: %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0memsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m             \u001b[0mprint_exec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 162\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPicklingError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msave_memoryview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPicklingError\u001b[0m: Could not serialize object: Exception: It appears that you are attempting to broadcast an RDD or reference an RDD from an action or transformation. RDD transformations and actions can only be invoked by the driver, not inside of other transformations; for example, rdd1.map(lambda x: rdd2.values.count() * x) is invalid because the values transformation and count action cannot be performed inside of the rdd1.map transformation. For more information, see SPARK-5063."
     ]
    }
   ],
   "source": [
    "# EXERCICIO\n",
    "\n",
    "import re\n",
    "import datetime\n",
    "from collections import namedtuple\n",
    "\n",
    "headeritems = header.split(',') # transformar o cabeçalho em lista\n",
    "del headeritems[-1] # apagar o último item e...\n",
    "headeritems[-1] = 'COORD' # transformar em COORD\n",
    "\n",
    "# Dates,Category,Descript,DayOfWeek,PdDistrict,Resolution,Address,COORD\n",
    "Crime = namedtuple('Crime',headeritems) # gera a namedtuple Crime com os campos de header\n",
    "\n",
    "REGEX = r',(?=(?:[^\"]*\"[^\"]*\")*(?![^\"]*\"))'\n",
    "# buscar por \",\" tal que após essa vírgula (?=) ou exista um par de \"\" ou não tenha \" sozinha\n",
    "# ?= indica para procurarmos pelo padrão após a vírgula\n",
    "# ?: significa para não interpretar os parênteses como captura de valores\n",
    "# [^\"]* 0 ou sequências de caracteres que não sejam aspas\n",
    "# [^\"]*\"[^\"]*\"  <qualquer caracter exceto aspas> \" <qualquer caracter exceto aspas> \"\n",
    "# ?! indica para verificar se não existe tal padrão a frente da vírgula\n",
    "\n",
    "\n",
    "def ParseCrime(rec):\n",
    "    # utilizando re.split() vamos capturar nossos valores\n",
    "    Date, Category, Descript, DayOfWeek, PdDistrict, Resolution, Address, X, Y = tuple(re.split(REGEX,rec))\n",
    "    \n",
    "    # Converta a data para o formato datetime\n",
    "    Date = datetime.datetime.strptime(Date, \"%Y-%m-%d %H:%M:%S\")\n",
    "    \n",
    "    # COORD é uma tupla com floats representando X e Y\n",
    "    COORD = (X,Y)\n",
    "    \n",
    "    # O campos 'Resolution' será uma lista dos valores separados por vírgula, sem as aspas\n",
    "    Resolution = CrimeHeadlessRDD(lambda x : x[5] )\n",
    "    return Crime(Date, Category, Descript, DayOfWeek, PdDistrict, Resolution, Address, COORD)\n",
    "\n",
    "# Aplique a função ParseCrime para cada objeto da base\n",
    "CrimeHeadlessRDD = (CrimeRDD\n",
    "                    .filter(lambda x: x!=header)\n",
    "                    .map(ParseCrime)\n",
    "                    )\n",
    "\n",
    "firstClean = CrimeHeadlessRDD.take(1)[0]\n",
    "totalRecs = CrimeHeadlessRDD.count()\n",
    "print firstClean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'firstClean' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-58-86eaa4de76b5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32massert\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfirstClean\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDates\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatetime\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfirstClean\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mResolution\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mlist\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfirstClean\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCOORD\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'tipos incorretos'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0;34m\"OK\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32massert\u001b[0m \u001b[0mCrimeHeadlessRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m!=\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'algo deu errado!'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0;34m\"OK\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'firstClean' is not defined"
     ]
    }
   ],
   "source": [
    "assert type(firstClean.Dates) is datetime.datetime and type(firstClean.Resolution) is list and type(firstClean.COORD) is tuple,'tipos incorretos'\n",
    "print \"OK\"\n",
    "\n",
    "assert CrimeHeadlessRDD.filter(lambda x: len(x)!=8).count()==0, 'algo deu errado!'\n",
    "print \"OK\"\n",
    "\n",
    "assert totalRecs==878049, 'total de registros incorreto'\n",
    "print \"OK\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ** Parte 2: Estatísticas Básicas das Variáveis **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Nessa parte do notebook vamos aprender a filtrar a base de dados para calcular estatísticas básicas necessárias para a análise exploratória."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ** (2a) Contagem de frequência **\n",
    "\n",
    "#### A contagem de frequência é realizada de forma similar ao exercício de contagem de palavras. Primeiro mapeamos a variável de interesse. Como exemplo vamos gerar uma lista da quantidade total de cada tipo de crime (Category)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXERCICIO\n",
    "\n",
    "CatCountRDD = (CrimeHeadlessRDD\n",
    "               .<COMPLETAR>\n",
    "               .<COMPLETAR>\n",
    "               )\n",
    "catCount = sorted(CatCountRDD.collect(), key=lambda x: -x[1])\n",
    "print catCount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "assert catCount[0][1]==174900, 'valores incorretos'\n",
    "print \"OK\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### De forma similar, vamos gerar a contagem para as regiões de São Francisco (PdDistrict)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXERCICIO\n",
    "\n",
    "RegionCountRDD = (CrimeHeadlessRDD\n",
    "                  .<COMPLETAR>\n",
    "                  .<COMPLETAR>\n",
    "                 )\n",
    "regCount = sorted(RegionCountRDD.collect(), key=lambda x: -x[1])\n",
    "print regCount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert regCount[0][1]==157182, 'valores incorretos'\n",
    "print \"OK\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(2b) Cálculo da Média**\n",
    "\n",
    "#### Nesse exercício vamos calcular a média de crimes em cada região para cada dia da semana. Para isso, primeiro devemos calcular a quantidade de dias de cada dia da semana que existem na base de dados, para isso vamos criar uma RDD de tuplas em que o primeiro campo é a tupla da data no formato 'dia-mes-ano' e do dia da semana e o segundo campo o valor $1$.\n",
    "\n",
    "#### Em seguida, reduzimos a RDD sem efetuar a soma, mantendo o valor $1$. Essa redução filtra a RDD para que cada data apareça uma única vez. Ao final,  podemos efetuar o mapeamento de (DayOfWeek,1) e redução com soma para contabilizar quantas vezes cada dia da semana aparece na base de dados.\n",
    "\n",
    "#### Nossa próxima RDD terá como chave uma tupla ( (DayOfWeek, PdDistrict), 1) para contabilizar quantos crimes ocorreram em determinada região e naquele dia da semana. Após a redução, devemos mapear esse RDD para (DayOfWeek, (PdDistrict, contagem)).\n",
    "\n",
    "#### Finalmente, podemos juntar as duas RDDs uma vez que elas possuem a mesma chave (DayOfWeek), dessa forma teremos tuplas no formato ( DayOfWeek, ( (PdDistrict,contagem), contagemDiaDaSemana ) ). Isso deve ser mapeado para:\n",
    "\n",
    "#### ( DayOfWeek, ( PdDistrict, contagem / contagemDiaDaSemana ) )\n",
    "\n",
    "#### Lembrando de converter `contagemDiaDaSemana` para `float`. Finalmente, o resultado pode ser agrupado pela chave, gerando uma tupla ( DayOfWeek, [ (Pd1, media1), (Pd2, media2), ... ] ). Essa lista pode ser mapeada para um dicionário com o comando `dict`.\n",
    "\n",
    "#### No final, transformamos o RDD em um dicionário Python com o comando `collectAsMap()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXERCICIO\n",
    "\n",
    "from operator import add\n",
    "\n",
    "# Dates,Category,Descript,DayOfWeek,PdDistrict,Resolution,Address,COORD\n",
    "\n",
    "# Lambda para converter um datetime em `Dia-Mes-Ano`\n",
    "day2str = lambda x: '{}-{}-{}'.format(x.day,x.month,x.year)\n",
    "\n",
    "totalDatesRDD = (CrimeHeadlessRDD\n",
    "                 .<COMPLETAR>\n",
    "                 .<COMPLETAR>\n",
    "                 .<COMPLETAR>\n",
    "                 .<COMPLETAR>\n",
    "                 )\n",
    "\n",
    "\n",
    "crimesWeekDayRegionRDD = (CrimeHeadlessRDD\n",
    "                           .<COMPLETAR>\n",
    "                           .<COMPLETAR>\n",
    "                           .<COMPLETAR>\n",
    "                          )\n",
    "\n",
    "RegionAvgPerDayRDD = (crimesWeekDayRegionRDD\n",
    "                      .<COMPLETAR>\n",
    "                      .<COMPLETAR>\n",
    "                      .<COMPLETAR>\n",
    "                      .<COMPLETAR>\n",
    "                     )\n",
    "\n",
    "RegionAvg = RegionAvgPerDayRDD.collectAsMap()\n",
    "print RegionAvg['Sunday']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert np.round(RegionAvg['Sunday']['BAYVIEW'],2)==37.27, 'valores incorretos {}'.format(np.round(RegionAvg[0][2],2))\n",
    "print \"OK\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ** (2c) Média e Desvio-Padrão pelo PySpark **\n",
    "\n",
    "#### Uma alternativa para calcular média, desvio-padrão e outros valores descritivos é utilizando os comandos internos do Spark. Para isso é necessário gerar uma RDD de listas de valores.\n",
    "\n",
    "#### Gere uma RDD contendo a tupla ( (Dates,DayOfWeek, PdDistrict), contagem), mapeie para ( (DayOfWeek,PdDistrict), Contagem) e agrupe pela chave. Isso irá gerar uma RDD ( (DayOfWeek,PdDistrict), Iterador(contagens) ).\n",
    "\n",
    "#### Agora crie um dicionário RegionAvgSpark, inicialmente vazio e colete apenas o primeiro elemento da tupla para a variável `Keys`. Itere essa variável realizando os seguintes passos:\n",
    "\n",
    "* #### Se `key[0]` não existir no dicionário, crie a entrada `key[0]` como um dicionário vazio.\n",
    "* #### Mapeie countWeekDayDistRDD filtrando por `key` e gerando a RDD com os valores da tupla. Note que não queremos uma lista de listas.\n",
    "* #### Insira a tupla (media, desvio-padrão) utilizando os comandos `mean()` e `stdev()` do PySpark, armazenando na chave RegionAvgSpark[ key[0] ][ key[1] ]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXERCICIO\n",
    "\n",
    "countWeekDayDistRDD = (CrimeHeadlessRDD\n",
    "                       .<COMPLETAR>\n",
    "                       .<COMPLETAR>\n",
    "                       .<COMPLETAR>\n",
    "                       .<COMPLETAR>\n",
    "                       )\n",
    "\n",
    "# Esse procedimento só é viável se existirem poucas chaves\n",
    "RegionAvgSpark = {}\n",
    "Keys = countWeekDayDistRDD.map(lambda rec: rec[0]).collect()\n",
    "for key in Keys:\n",
    "    listRDD = (countWeekDayDistRDD\n",
    "               .filter(lambda rec: rec[0]==key)\n",
    "               .flatMap(lambda rec: rec[1])\n",
    "               )\n",
    "    if key[0] not in RegionAvgSpark:\n",
    "        RegionAvgSpark[key[0]] = {}    \n",
    "    RegionAvgSpark[key[0]][key[1]] = (listRDD.mean(), listRDD.stdev())\n",
    "    \n",
    "print RegionAvgSpark['Sunday']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert np.round(RegionAvgSpark['Sunday']['BAYVIEW'][0],2)==37.39 and np.round(RegionAvgSpark['Sunday']['BAYVIEW'][1],2)==10.06, 'valores incorretos'\n",
    "print \"OK\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parte 3: Plotagem de Gráficos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Nessa parte do notebook vamos aprender a manipular os dados para gerar listas de valores a serem utilizados na plotagem de gráficos.\n",
    "\n",
    "#### Para a plotagem de gráficos vamos utilizar o [`matplotlib`](http://matplotlib.org/) que já vem por padrão na maioria das distribuições do Python (ex.: Anaconda). Outras bibliotecas alternativas interessantes são: [Seaborn](http://stanford.edu/~mwaskom/software/seaborn/) e [Bokeh](http://bokeh.pydata.org/en/latest/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ** (3a) Gráfico de Barras **\n",
    "\n",
    "#### O gráfico de barras é utilizado quando queremos comparar dados entre categorias diferentes de uma variável categórica. Como exemplo, vamos contabilizar o número médio de crimes diários por região.\n",
    "\n",
    "#### Vamos primeiro criar a RDD totalDatesRDD que contém a lista de dias únicos, computaremos o total de dias com o comando `count()` armazenando na variável `totalDays`. Não se esqueça de converter o valor para `float`.\n",
    "\n",
    "#### Em seguida, crie o RDD avgCrimesRegionRDD que utiliza a RDD RegionCountRDD para calcular a média de crimes por região.\n",
    "\n",
    "#### Utilizando o comando `zip()` do Python é possível descompactar um dicionário em duas variáveis, uma com as chaves e outra com os valores. Utilizaremos essas variáveis para a plotagem do gráfico."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Dates,Category,Descript,DayOfWeek,PdDistrict,Resolution,Address,COORD\n",
    "\n",
    "# Lambda para converter um datetime em `Dia-Mes-Ano`\n",
    "day2str = lambda x: '{}-{}-{}'.format(x.day,x.month,x.year)\n",
    "\n",
    "totalDatesRDD = (CrimeHeadlessRDD\n",
    "                 .map(lambda rec: (day2str(rec.Dates),1))\n",
    "                 .reduceByKey(lambda x,y: x)\n",
    "                 )\n",
    "\n",
    "totalDays = float(totalDatesRDD.count())\n",
    "\n",
    "avgCrimesRegionRDD = (RegionCountRDD\n",
    "                      .map(lambda rec: (rec[0],rec[1]/totalDays))\n",
    "                     )\n",
    "\n",
    "Xticks,Y = zip(*avgCrimesRegionRDD.collectAsMap().items())\n",
    "indices = np.arange(len(Xticks))\n",
    "width = 0.35\n",
    "\n",
    "fig = plt.figure(figsize=(8,4.2), facecolor='white', edgecolor='white')\n",
    "plt.bar(indices,Y, width)\n",
    "plt.grid(b=True, which='major', axis='y')\n",
    "plt.xticks(indices+width/2., Xticks, rotation=17 )\n",
    "plt.ylabel('Number of crimes')\n",
    "plt.xlabel('Region')\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Quando temos subcategorias de interesse, podemos plotar através de um gráfico de barras empilhado. Vamos plotar o conteúdo da variável RegionAvg.\n",
    "\n",
    "#### Primeiro passo é criar um dicionário `Y` em que a chave é o dia da semana e o valor é uma `np.array` contendo a média de cada região para aquele dia.\n",
    "\n",
    "#### Em seguida precisamos criar uma matriz `Bottom` que determina qual é o início de cada uma das barras. O início da barra do dia `i` deve ser o final da barra do dia `i-1`.\n",
    "\n",
    "#### Com isso calculado podemos gerar um plot por dia com o parâmetro bottom correspondente ao vetor `Bottom` daquele dia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dias da semana como referência\n",
    "Day = ['Sunday','Monday','Tuesday','Wednesday','Thursday','Friday','Saturday']\n",
    "\n",
    "# Uma cor para cada dia\n",
    "Color = ['r','b','g','y','c','k','purple']\n",
    "\n",
    "# Dicionário (dia, array de médias)\n",
    "Y = {}\n",
    "for day in Day:\n",
    "    Y[day] = np.array([RegionAvg[day][x] for x in Xticks])\n",
    "\n",
    "# Matriz dias x regiões    \n",
    "Bottom = np.zeros( (len(Day),len(Xticks)) )\n",
    "for i in range(1,len(Day)):\n",
    "    Bottom[i,:] = Bottom[i-1,:]+Y[Day[i-1]]\n",
    "    \n",
    "indices = np.arange(len(Xticks))\n",
    "width = 0.35\n",
    "\n",
    "fig = plt.figure(figsize=(8,4.2), facecolor='white', edgecolor='white')\n",
    "\n",
    "# Gera uma lista de plots, um para cada dia\n",
    "plots = [plt.bar(indices,Y[Day[i]], width, color=Color[i], bottom=Bottom[i]) for i in range(len(Day))]\n",
    "\n",
    "plt.legend( [p[0] for p in plots], Day,loc='center left', bbox_to_anchor=(1, 0.5) ) \n",
    "    \n",
    "plt.grid(b=True, which='major', axis='y')\n",
    "plt.xticks(indices+width/2., Xticks, rotation=17 )\n",
    "plt.ylabel('Number of crimes')\n",
    "plt.xlabel('Region')\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ** (3b) Gráfico de Linha **\n",
    "\n",
    "#### O gráfico de linha é utilizado principalmente para mostrar uma tendência temporal.\n",
    "\n",
    "#### Nesse exercício vamos primeiro gerar o número médio de crimes em cada hora do dia.\n",
    "\n",
    "#### Primeiro, novamente, geramos um RDD contendo um único registro de cada hora para cada dia. Em seguida, contabilizamos a soma da quantidade de crime em cada hora. Finalmente, juntamos as duas RDDs e calculamos a média dos valores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXERCICIO\n",
    "\n",
    "parseWeekday = lambda x: '{}-{}-{}'.format(x.day, x.month, x.year)\n",
    "\n",
    "hoursRDD = (CrimeHeadlessRDD\n",
    "            .<COMPLETAR>\n",
    "            .<COMPLETAR>\n",
    "            .<COMPLETAR>\n",
    "            .<COMPLETAR>\n",
    "           )\n",
    "\n",
    "crimePerHourRDD = (CrimeHeadlessRDD\n",
    "                   .<COMPLETAR>\n",
    "                   .<COMPLETAR>\n",
    "                  )\n",
    "\n",
    "avgCrimeHourRDD = (crimePerHourRDD\n",
    "                   .<COMPLETAR>\n",
    "                   .<COMPLETAR>\n",
    "                  )\n",
    "\n",
    "crimePerHour = avgCrimeHourRDD.collect()\n",
    "print crimePerHour[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert np.round(crimePerHour[0][1],2)==19.96, 'valores incorretos'\n",
    "print \"OK\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "crimePerHourSort = sorted(crimePerHour,key=lambda x: x[0])\n",
    "\n",
    "X,Y = zip(*crimePerHourSort)\n",
    "\n",
    "fig = plt.figure(figsize=(8,4.2), facecolor='white', edgecolor='white')\n",
    "plt.plot(X,Y)\n",
    "plt.grid(b=True, which='major', axis='y')\n",
    "plt.ylabel('Avg. Number of crimes')\n",
    "plt.xlabel('Hour')\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(3c) Gráfico de Dispersão**\n",
    "\n",
    "#### O gráfico de dispersão é utilizado para visualizar correlações entre as variáveis. Com esse gráfico é possível observar se o crescimento da quantidade de uma categoria está relacionada ao crescimento/decrescimento de outra (mas não podemos dizer se uma causa a outra).\n",
    "\n",
    "#### Na primeira parte do exercício calcularemos a correlação entre os diferentes tipos de crime. Para isso primeiro precisamos construir uma RDD em que cada registro corresponde a uma data o valor contido nele é a quantidade de crimes de cada tipo.\n",
    "\n",
    "#### Diferente dos exercícios anteriores, devemos manter essa informação como uma lista de valores em que todos os registros sigam a mesma ordem da lista de crimes.\n",
    "\n",
    "#### O primeiro passo é criar uma RDD com a tupla ( (Mes-Ano, Crime), 1 ) e utilizá-la para gerar a tupla ( (Mes-Ano,Crime) Quantidade ).\n",
    "\n",
    "#### Mapeamos essa RDD para definir Mes-Ano como chave e agrupamos em torno dessa chave, gerando uma lista de quantidade de crimes em cada data. Aplicamos a função `dict()` nessa lista para obtermos uma RDD no seguinte formato: (Mes-Ano, {CRIME: quantidade}).\n",
    "\n",
    "#### Além disso, vamos criar a variável `crimes` contendo a lista de crimes contidas na lista de pares `catCount` computada anteriormente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXERCICIO\n",
    "parseMonthYear = lambda x: '{}-{}'.format(x.month, x.year)\n",
    "\n",
    "crimes = map(lambda x: x[0], catCount)\n",
    "\n",
    "datesCrimesRDD = (CrimeHeadlessRDD\n",
    "                  .<COMPLETAR>\n",
    "                  .<COMPLETAR>\n",
    "                  .<COMPLETAR>\n",
    "                  .<COMPLETAR>\n",
    "                  .<COMPLETAR>\n",
    "                  .cache()\n",
    "                 )\n",
    "\n",
    "print datesCrimesRDD.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert datesCrimesRDD.take(1)[0][1][u'KIDNAPPING']==12,'valores incorretos'\n",
    "print 'ok'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### O próximo passo consiste em calcular o total de pares Mes-Ano para ser possível o cálculo da média.\n",
    "\n",
    "#### Finalmente, criamos a RDD `fractionCrimesDateRDD` em que a chave é Mes-Ano e o valor é uma lista da fração de cada tipo de crime ocorridos naquele mês e ano. Para gerar essa lista vamos utilizar o *list comprehension* do Python de tal forma a calcular a fração para cada crime na variável `crimes`.\n",
    "\n",
    "#### Os dicionários em Python tem um método chamado `get()` que permite atribuir um valor padrão caso a chave não exista. Ex.: `dicionario.get( chave, 0.0)` retornará 0.0 caso a chave não exista."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# EXERCICIO\n",
    "\n",
    "totalPerDateRDD = (CrimeHeadlessRDD\n",
    "                   .<COMPLETAR>\n",
    "                   .<COMPLETAR>\n",
    "                  )\n",
    "\n",
    "fractionCrimesDateRDD = (datesCrimesRDD\n",
    "                         .<COMPLETAR>\n",
    "                         .<COMPLETAR>\n",
    "                         .cache()\n",
    "                        )\n",
    "\n",
    "print fractionCrimesDateRDD.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert np.abs(fractionCrimesDateRDD.take(1)[0][1][0][1]-0.163950)<1e-6,'valores incorretos'\n",
    "print 'ok'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finalmente, utilizaremos a função `Statistics.corr()` da biblioteca [`pyspark.mlllib.stat`](https://spark.apache.org/docs/1.1.0/api/python/pyspark.mllib.stat.Statistics-class.html).\n",
    "\n",
    "#### Para isso mapeamos nossa RDD para conter apenas a lista de valores da lista de tuplas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.stat import Statistics\n",
    "corr = Statistics.corr(fractionCrimesDateRDD.map(lambda rec: map(lambda x: x[1],rec[1])))\n",
    "print corr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convertendo a matriz `corr` para `np.array` podemos buscar pelo maior valor negativo e positivo diferentes de 1.0. Para isso vamos utilizar as funções `min()` e `argmin()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "npCorr = np.array(corr)\n",
    "rowMin = npCorr.min(axis=1).argmin()\n",
    "colMin = npCorr[rowMin,:].argmin()\n",
    "print crimes[rowMin], crimes[colMin], npCorr[rowMin,colMin]\n",
    "\n",
    "npCorr[npCorr==1.] = 0.\n",
    "rowMax = npCorr.max(axis=1).argmax()\n",
    "colMax = npCorr[rowMax,:].argmax()\n",
    "print crimes[rowMax], crimes[colMax], npCorr[rowMax,colMax]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Agora que sabemos quais crimes tem maior correlação, vamos plotar um gráfico de dispersão daqueles com maior correlação negativa.\n",
    "\n",
    "#### Primeiro criamos duas RDDs, `var1RDD` e `var2RDD`. Elas são um mapeamento da `fractionCrimesDateRDD` filtradas para conter apenas o crime contido em Xlabel e Ylabel, respectivamente.\n",
    "\n",
    "#### Juntamos as duas RDDs em uma única RDD, `correlationRDD` que mapeará para tuplas de valores, onde os valores são as médias calculadas em fractionCrimesDateRDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXERCICIO\n",
    "\n",
    "Xlabel = 'FORGERY/COUNTERFEITING'#'DRIVING UNDER THE INFLUENCE'\n",
    "Ylabel = 'NON-CRIMINAL'#'LIQUOR LAWS'\n",
    "\n",
    "\n",
    "\n",
    "var1RDD = (fractionCrimesDateRDD\n",
    "           .map(lambda rec: (rec[0], filter(lambda x: x[0]==Xlabel,rec[1])[0][1]))\n",
    "          )\n",
    "var2RDD = (fractionCrimesDateRDD\n",
    "           .map(lambda rec: (rec[0], filter(lambda x: x[0]==Ylabel,rec[1])[0][1]))\n",
    "          )\n",
    "\n",
    "correlationRDD = (var1RDD\n",
    "                  .<COMPLETAR>\n",
    "                  .<COMPLETAR>\n",
    "                 )\n",
    "\n",
    "\n",
    "Data = correlationRDD.collect()\n",
    "print Data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert np.abs(Data[0][0]-0.015904)<1e-6, 'valores incorretos'\n",
    "print 'ok'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### No gráfico abaixo, é possível perceber que quanto mais crimes do tipo *NON-CRIMINAL* ocorrem em um dia, menos *FORGERY/COUNTERFEITING* ocorrem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,Y = zip(*Data)\n",
    "\n",
    "fig = plt.figure(figsize=(8,4.2), facecolor='white', edgecolor='white')\n",
    "plt.scatter(X,Y)\n",
    "plt.grid(b=True, which='major', axis='y')\n",
    "plt.xlabel(Xlabel)\n",
    "plt.ylabel(Ylabel)\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(3d) Histograma **\n",
    "\n",
    "#### O uso do Histograma é para visualizar a distribuição dos dados. Dois tipos de distribuição que são observadas normalmente é a Gaussiana, em que os valores se concentram em torno de uma média e a Lei de Potência, em que os valores menores são observados com maior frequência.\n",
    "\n",
    "#### Vamos verificar a distribuição das prisões efetuadas (categoria *ARREST* em * Resolution*) em cada mês. Com essa distribuição poderemos verificar se o número de prisões é consistente durante os meses do período estudado.\n",
    "\n",
    "#### Primeiro criaremos uma RDD chamada `bookedRDD` que contém apenas os registros contendo *ARREST* no campo *Resolution* (lembre-se que esse campo é uma lista) e contabilizar a quantidade de registros em cada 'Mes-Ano'. Ao final, vamos mapear para uma RDD contendo apenas os valores contabilizados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXERCICIO\n",
    "\n",
    "bookedRDD = (CrimeHeadlessRDD\n",
    "             .<COMPLETAR>\n",
    "             .<COMPLETAR>\n",
    "             .<COMPLETAR>\n",
    "             .<COMPLETAR>\n",
    "            )\n",
    "\n",
    "Data = bookedRDD.collect()\n",
    "print Data[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert Data[0]==1914,'valores incorretos'\n",
    "print 'ok'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8,4.2), facecolor='white', edgecolor='white')\n",
    "plt.hist(Data)\n",
    "plt.grid(b=True, which='major', axis='y')\n",
    "plt.xlabel('ARRESTED')\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Notem que lemos o histograma da seguinte maneira: em cerca de 50 meses foram observadas entre 1750 e 2000 prisões. Porém, não sabemos precisar em quais meses houve um aumento ou redução das prisões. Isso deve ser observado através de um gráfico de linha."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(3e) Box-plot**\n",
    "\n",
    "#### O Box-plot é um gráfico muito utilizado em estatística para visualizar o resumo estatístico de uma variável.\n",
    "\n",
    "#### Para esse exercício vamos plotar duas box-plot sobre a média do número de prisões durante os meses analisados para os crimes do tipo *ROBBERY* e *ASSAULT*.\n",
    "\n",
    "#### O mapeamento é exatamente o mesmo do exercício anterior, porém filtrando para o tipo de roubo analisado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# EXERCICIO\n",
    "\n",
    "parseDayMonth = lambda x: '{}-{}'.format(x.month,x.year)\n",
    "\n",
    "robberyBookedRDD = (CrimeHeadlessRDD\n",
    "                     .<COMPLETAR>\n",
    "                     .<COMPLETAR>\n",
    "                     .<COMPLETAR>\n",
    "                     .<COMPLETAR>\n",
    "                    )\n",
    "\n",
    "assaultBookedRDD = (CrimeHeadlessRDD\n",
    "                     .<COMPLETAR>\n",
    "                     .<COMPLETAR>\n",
    "                     .<COMPLETAR>\n",
    "                     .<COMPLETAR>\n",
    "                    )\n",
    "\n",
    "robData = robberyBookedRDD.collect()\n",
    "assData = assaultBookedRDD.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert robData[0]==27,'valores incorretos'\n",
    "print 'ok'\n",
    "assert assData[0]==152,'valores incorretos'\n",
    "print 'ok'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### No gráfico abaixo, percebemos que existem, em média, muito mais prisões para o tipo *ASSAULT* do que o tipo *ROBBERY*, ambos com pequena variação."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8,4.2), facecolor='white', edgecolor='white')\n",
    "plt.boxplot([robData,assData])\n",
    "plt.grid(b=True, which='major', axis='y')\n",
    "plt.ylabel('ARRESTED')\n",
    "plt.xticks([1,2], ['ROBBERY','ASSAULT'])\n",
    "pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
